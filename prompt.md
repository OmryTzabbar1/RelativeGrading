# Student Project Evaluator - Context Summary

## Project Overview

This is a **Claude Code skill** for comparative evaluation of student coding projects. It implements a dynamic baseline grading system where each student project is evaluated relative to the highest-graded project encountered so far.

**Repository**: https://github.com/OmryTzabbar1/RelativeGrading.git

## Core Concept: Comparative Grading with Dynamic Baseline

Instead of absolute grading, this skill:
1. Instructor manually grades one "baseline" student (e.g., 85/100)
2. System analyzes baseline features and quality level
3. Each subsequent student is compared to the current best project
4. If a student scores higher than baseline, they become the new baseline
5. All grades are relative: `final_grade = baseline_grade Â± adjustments`

**Example Flow:**
```
Baseline: Alice (85/100)
â†“
Bob evaluated: +10 relevance, +10 functionality = 105 â†’ 100/100
â†“
New Baseline: Bob (100/100)
â†“
Carol evaluated: -10 relevance, -10 functionality = 80/100
```

## Project Structure

```
E:/Projects/student-project-evaluator/
â”œâ”€â”€ .claude/
â”‚   â””â”€â”€ skills/
â”‚       â”œâ”€â”€ student-project-evaluator.md  # Main skill definition (437 lines)
â”‚       â”œâ”€â”€ analyze_repo.py               # Project type detection (150 lines)
â”‚       â””â”€â”€ scan_folders.py               # Student folder discovery (99 lines)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PRD.md           # Product requirements (generated by project-docs-generator)
â”‚   â”œâ”€â”€ PLANNING.md      # Architecture, C4 diagrams, ADRs
â”‚   â”œâ”€â”€ CLAUDE.md        # Development rules (150-line file limit, type hints required)
â”‚   â””â”€â”€ TASKS.md         # 5-phase implementation plan
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ fixtures/
â”‚       â””â”€â”€ sample_assignment/
â”‚           â”œâ”€â”€ student_alice/     # Baseline: 85/100
â”‚           â”œâ”€â”€ student_bob/       # Superior: 100/100
â”‚           â””â”€â”€ student_carol/     # Incomplete: 80/100
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ assignment-config.yml      # Configuration template
â”œâ”€â”€ requirements.txt               # pyyaml>=6.0
â”œâ”€â”€ README.md                      # Complete project documentation
â”œâ”€â”€ LICENSE                        # MIT License
â””â”€â”€ prompt.md                      # This file

Git remote: origin https://github.com/OmryTzabbar1/RelativeGrading.git
Branch: master (up to date)
```

## How the Skill Works

### Execution Flow (5 Steps)

The skill is defined in `.claude/skills/student-project-evaluator.md` with this workflow:

#### **Step 1: Gather Required Information**
Prompt user for:
- Assignment description/requirements
- Master folder path containing all student project folders
- Baseline student name, folder path, and grade (0-100)

Validate inputs (paths exist, grade in range).

#### **Step 2: Calibrate Baseline**
1. Use `analyze_repo.py` to detect project type (JavaScript/Python/Java)
2. Read baseline student's key files (README, source code, configs)
3. Analyze features, code quality, and assignment alignment
4. Generate `assignment-config.yml` in master folder with:
   - Assignment details
   - Baseline features observed
   - Grading weights (50% relevance, 50% functionality)
   - Focus file patterns
   - Evaluation criteria

#### **Step 3: Discover Student Folders**
Use `scan_folders.py` to find all student folders (excluding baseline).

#### **Step 4: Batch Evaluation Loop**
Process students in **batches of 5** to manage context:

For each student:
1. **Read project**: Use `analyze_repo.py` for metadata, read key files
2. **Compare to current baseline**:
   - **Assignment Relevance (50%)**: How well requirements are met
     - Adjustment: -10 to +20 points
   - **Functionality (50%)**: Implementation quality, code organization, error handling
     - Adjustment: -10 to +20 points
3. **Calculate grade**: `baseline_grade + relevance_adj + functionality_adj` (clamped 0-100)
4. **Update baseline if better**: If `final_grade > baseline_grade`, this becomes new baseline
5. **Store evaluation**: Student name, ID, team, grade, reasoning
6. **Error handling**: Skip and flag for manual review if issues occur

#### **Step 5: Generate Outputs**

Create three files in master folder:

1. **grades.csv** - Gradebook import format:
   ```csv
   Student,ID,Grade,Rank,Team,Notes
   student_bob,67890,100,1,Beta Squad,"Exceptional implementation..."
   ```

2. **evaluation_summary.md** - Detailed report with:
   - Statistics (mean, median, std dev)
   - Grade distribution histogram
   - Individual evaluations with reasoning
   - Students requiring manual review

3. **assignment-config.yml** - Generated configuration (already created in Step 2)

## Helper Scripts

Located in `.claude/skills/`:

### `analyze_repo.py`
Detects project type and extracts metadata.

```python
from analyze_repo import analyze_repository

info = analyze_repository('/path/to/student/folder')
# Returns:
# {
#   'project_type': 'javascript_web' | 'python_app' | 'java_app' | 'unknown',
#   'key_files': ['README.md', 'src/index.js', 'package.json'],
#   'metadata': {'team_name': 'Team Alpha', 'student_id': '12345'},
#   'size_mb': 3.5,
#   'file_count': 89
# }
```

**Detection logic:**
- JavaScript: `package.json` + (`index.js` or `app.js`)
- Python: `requirements.txt` or `setup.py` + (`main.py` or `app.py`)
- Java: `pom.xml` or `build.gradle`

### `scan_folders.py`
Finds all student subdirectories in master folder.

```python
from scan_folders import scan_student_folders

students = scan_student_folders('/master/folder', exclude=['baseline_student'])
# Returns:
# [
#   {'name': 'student_bob', 'path': '/master/folder/student_bob'},
#   {'name': 'student_carol', 'path': '/master/folder/student_carol'}
# ]
```

Filters out hidden folders (starting with `.`) and excluded folders.

## Grading Methodology

### Evaluation Criteria (50/50 split)

1. **Assignment Relevance (50%)**:
   - Does project address requirements?
   - Are required features implemented?
   - Goes beyond or falls short of requirements?
   - Scale: -10 to +20 points from baseline

2. **Functionality (50%)**:
   - Feature completeness vs baseline
   - Code organization and structure
   - Documentation quality
   - Error handling and validation
   - Scale: -10 to +20 points from baseline

### Adjustment Ranges

- **-10 points**: Significantly worse than baseline (missing features, poor quality)
- **0 points**: Comparable to baseline
- **+10 points**: Better than baseline (more features, better quality)
- **+20 points**: Significantly exceeds baseline (exceptional work)

### Grade Clamping

Final grades are clamped to 0-100 range:
```python
final_grade = max(0, min(100, baseline_grade + total_adjustments))
```

## Development Workflow (from WorkEnv)

This project was built using the **WorkEnv methodology**:

1. **Project Discovery**: 9-phase interview (project-discovery skill)
2. **Generate Docs**: Create PRD, PLANNING, CLAUDE, TASKS (project-docs-generator skill)
3. **Implementation**: Follow generated documentation
4. **Commit Convention**: `<type>(<scope>): <description> [TaskID]`

### Key Development Rules (from docs/CLAUDE.md)

- âœ… Maximum 150 lines per file (strictly enforced)
- âœ… Type hints required for all Python functions
- âœ… PEP 8 style guide
- âœ… Docstrings for all public functions
- âœ… 70%+ test coverage requirement

## Current Project Status

### âœ… Completed Phases

- **Phase 1**: Project structure, helper scripts (`analyze_repo.py`, `scan_folders.py`)
- **Phase 2**: Main skill definition (`.claude/skills/student-project-evaluator.md`)
- **Phase 4 (Partial)**: Test fixtures created with 3 sample students
- **Phase 5**: README.md and LICENSE added
- **Phase 6**: Pushed to GitHub repository

### ðŸ“ Remaining Work

- **Phase 3**: Output generation code (currently handled by skill orchestration)
- **Phase 4**: Unit tests for Python scripts (70%+ coverage)
- **Phase 5**: Additional examples and documentation polish

### Last Test Run

Successfully evaluated 3 test students in `tests/fixtures/sample_assignment/`:
- **student_alice**: 85/100 (baseline)
- **student_bob**: 100/100 (superior - new baseline)
- **student_carol**: 80/100 (incomplete)

Generated outputs:
- `grades.csv`
- `evaluation_summary.md`
- `assignment-config.yml`

## How to Use This Skill

### Option 1: Invoke as Claude Code Skill (if registered)

```
Use the student-project-evaluator skill
```

### Option 2: Manual Execution

Follow the 5-step workflow from `.claude/skills/student-project-evaluator.md`:

1. Gather assignment info and baseline student
2. Analyze baseline with:
   ```bash
   cd E:/Projects/student-project-evaluator/.claude/skills
   python -c "from analyze_repo import analyze_repository; ..."
   ```
3. Scan students with:
   ```python
   from scan_folders import scan_student_folders
   students = scan_student_folders('/path/to/master', exclude=['baseline'])
   ```
4. Evaluate each student by reading files and comparing to baseline
5. Generate CSV and markdown outputs

### Example Usage

```python
# From .claude/skills/ directory
import sys
sys.path.insert(0, '.')

from scan_folders import scan_student_folders
from analyze_repo import analyze_repository

# Discover students
master_folder = '../../tests/fixtures/sample_assignment'
students = scan_student_folders(master_folder, exclude=['student_alice'])

# Analyze each
for student in students:
    info = analyze_repository(student['path'])
    print(f"{student['name']}: {info['project_type']}")
```

## Important Notes for Continuation

### File Locations
- **Skill definition**: `.claude/skills/student-project-evaluator.md` (NOT in root)
- **Helper scripts**: `.claude/skills/` (NOT in `scripts/` - we moved them)
- **Test data**: `tests/fixtures/sample_assignment/`

### Recent Refactoring
We moved Python scripts from `scripts/` to `.claude/skills/` for better cohesion. All imports updated:
- ~~`from scripts.analyze_repo import`~~ â†’ `from analyze_repo import`
- ~~`from scripts.scan_folders import`~~ â†’ `from scan_folders import`

### Git Status
- All changes committed and pushed to origin/master
- Working directory clean
- Last commit: "refactor: Move Python scripts to .claude/skills for better cohesion"

### Testing
To verify scripts work:
```bash
cd E:/Projects/student-project-evaluator/.claude/skills
python -c "from scan_folders import scan_student_folders; print(scan_student_folders('../../tests/fixtures/sample_assignment'))"
```

## Key Files to Read for Context

If you need full context, read these in order:

1. **README.md** - Complete project documentation
2. **docs/CLAUDE.md** - Development rules and conventions
3. **.claude/skills/student-project-evaluator.md** - Full skill workflow
4. **docs/PLANNING.md** - Architecture and ADRs
5. **docs/PRD.md** - Product requirements

## Quick Reference Commands

```bash
# Navigate to project
cd E:/Projects/student-project-evaluator

# Test helper scripts
cd .claude/skills
python -c "from scan_folders import scan_student_folders; ..."

# Check git status
git status

# Run skill manually (follow SKILL.md workflow)
# Read .claude/skills/student-project-evaluator.md for detailed steps

# View test results
cat tests/fixtures/sample_assignment/grades.csv
cat tests/fixtures/sample_assignment/evaluation_summary.md
```

## Dependencies

- Python 3.7+
- pyyaml>=6.0 (from requirements.txt)
- Claude Code CLI (for skill invocation)

## Contact & Support

- **Repository**: https://github.com/OmryTzabbar1/RelativeGrading
- **License**: MIT
- **Issues**: Open on GitHub

---

**Last Updated**: 2025-12-16
**Project Status**: Core functionality complete, ready for production use with test fixtures validated
