---
name: grade-comparison
description: Compare student-project-evaluator grades with actual grades from PDF files. Extracts grades from Detailed_Grade_Breakdown PDFs and creates Excel comparison reports with correlation analysis.
---

# Grade Comparison Tool

Compares grades generated by the student-project-evaluator with actual grades found in student submission folders.

## When to Use This Skill

- User wants to compare automated evaluation results with actual grades
- User asks to "compare grades", "validate evaluation", or "check accuracy"
- User needs correlation analysis between evaluator and actual grades
- User wants to identify discrepancies between automated and manual grading

## Quick Start

```
Comparison Workflow:
- [ ] Step 1: Locate evaluator grades (grades.xlsx in outputs/)
- [ ] Step 2: Get WorkSubmissions folder path from user
- [ ] Step 3: For each student, find grade PDF
- [ ] Step 4: Extract actual grade from PDF
- [ ] Step 5: Calculate comparison metrics
- [ ] Step 6: Generate Excel comparison report
```

## Input Requirements

1. **Evaluator Grades**: `outputs/grades.xlsx` (from student-project-evaluator)
2. **WorkSubmissions Folder**: Path to folder containing student submissions
3. **PDF Pattern**: `Detailed_Grade_Breakdown_{student_id}.pdf` in each student folder

**Expected structure:**
```
WorkSubmissions01/
├── Participant_38951_assignsubmission_file/
│   ├── Detailed_Grade_Breakdown_38951.pdf
│   └── ...
├── Participant_38953_assignsubmission_file/
│   ├── Detailed_Grade_Breakdown_38953.pdf
│   └── ...
└── ...
```

## Execution Flow

### Step 1: Load Evaluator Grades

Read the generated grades from `outputs/grades.xlsx`:
- Student ID
- Final grade (with rarity bonuses)
- Base percentage
- Criteria count

**Source:** Latest grades.xlsx from student-project-evaluator evaluation

### Step 2: Get WorkSubmissions Path

Ask the user for the WorkSubmissions folder path, or use the argument provided.

**Example:**
- `E:\Projects\student-project-evaluator\tests\WorkSubmissions01`

### Step 3: Find Grade PDFs

For each student:
1. Identify student ID from evaluator grades
2. Find corresponding folder: `Participant_{student_id}_assignsubmission_file/`
3. Locate PDF: `Detailed_Grade_Breakdown_{student_id}.pdf`

**Error handling:**
- Student folder not found → Log warning, mark as "Not Found"
- PDF not found → Log warning, mark as "No PDF"
- Continue with other students

### Step 4: Extract Actual Grade

Read PDF and extract the actual grade:

**Extraction methods:**
1. Look for "Total Grade:", "Final Grade:", "Overall Grade:"
2. Parse percentage (e.g., "85%", "85.5%", "85.5/100")
3. Handle various formats:
   - "Total Grade: 85.5%"
   - "Final Grade: 85.5/100"
   - "Grade: 85.5"

**Validation:**
- Grade should be 0-100
- If multiple grades found, use the highest/most prominent
- If no grade found, mark as "Could not parse"

### Step 5: Calculate Comparison Metrics

For each student:
```python
difference = evaluator_grade - actual_grade
abs_difference = abs(difference)
percentage_error = (abs_difference / actual_grade) * 100
```

**Aggregate metrics:**
- Mean difference
- Mean absolute difference
- Correlation coefficient (Pearson's r)
- Root mean squared error (RMSE)
- Number of students where evaluator > actual
- Number of students where evaluator < actual

### Step 6: Generate Excel Report

Create comprehensive Excel workbook: `outputs/grade_comparison.xlsx`

**Sheet 1: Comparison**
| Student ID | Evaluator Grade | Actual Grade | Difference | Abs Diff | % Error | Criteria |
|------------|-----------------|--------------|------------|----------|---------|----------|
| 38951 | 77.8 | 85.0 | -7.2 | 7.2 | 8.5% | 17/30 |
| 38953 | 61.6 | 70.0 | -8.4 | 8.4 | 12.0% | 11/30 |

**Sheet 2: Statistics**
- Mean Evaluator Grade
- Mean Actual Grade
- Mean Difference
- Mean Absolute Difference
- Correlation Coefficient
- RMSE
- % Students within ±5 points
- % Students within ±10 points

**Sheet 3: Discrepancies**
List students with large discrepancies (>15 points difference):
- Highlighting over-graded students (evaluator > actual)
- Highlighting under-graded students (evaluator < actual)

**Sheet 4: Charts**
- Scatter plot: Evaluator vs Actual grades
- Histogram: Distribution of differences
- Bar chart: Top 10 discrepancies

## Implementation

Use the `compare_grades.py` script:

```bash
cd .claude/skills/grade-comparison/scripts
python compare_grades.py <worksubmissions_path>
```

**Example:**
```bash
python compare_grades.py "E:\Projects\student-project-evaluator\tests\WorkSubmissions01"
```

The script will:
1. Load grades from `outputs/grades.xlsx`
2. Scan the WorkSubmissions folder
3. Extract grades from PDFs using PyPDF2
4. Generate comparison Excel report

## Output Format

### Console Output

```
================================================================================
GRADE COMPARISON REPORT
================================================================================

Loading evaluator grades from: outputs/grades.xlsx
Found 36 students in evaluator results

Scanning WorkSubmissions folder: E:\...\WorkSubmissions01
Found 36 student folders

Extracting actual grades from PDFs...
[1/36] Student 38951: Actual=85.0, Evaluator=77.8, Diff=-7.2
[2/36] Student 38953: Actual=70.0, Evaluator=61.6, Diff=-8.4
...

================================================================================
SUMMARY STATISTICS
================================================================================

Mean Evaluator Grade:   64.5
Mean Actual Grade:      72.3
Mean Difference:        -7.8 (evaluator grades LOWER on average)
Mean Absolute Error:    9.2
Correlation:            0.85 (strong positive correlation)
RMSE:                   10.4

Students within ±5 points:   12/36 (33.3%)
Students within ±10 points:  28/36 (77.8%)

================================================================================

Saved: outputs/grade_comparison.xlsx
```

### Excel File Structure

**outputs/grade_comparison.xlsx**

1. **Comparison** - Full student-by-student comparison
2. **Statistics** - Aggregate metrics and analysis
3. **Discrepancies** - Students with >15 point differences
4. **Charts** - Visual comparisons

## Error Handling

**Missing evaluator grades:**
- Error: "Could not find outputs/grades.xlsx"
- Solution: Run student-project-evaluator first

**WorkSubmissions folder not found:**
- Error: "WorkSubmissions path does not exist"
- Solution: Verify path and try again

**Student folder not found:**
- Log: "Warning: Folder not found for student {id}"
- Mark as "Not Found" in comparison
- Continue with other students

**PDF not found:**
- Log: "Warning: No PDF found for student {id}"
- Mark as "No PDF" in comparison
- Continue with other students

**Grade parsing failed:**
- Log: "Warning: Could not parse grade from PDF for student {id}"
- Mark as "Parse Failed" in comparison
- Continue with other students

**No matching students:**
- Error: "No students could be matched between evaluator and actual grades"
- Solution: Verify student IDs match between sources

## Example Session

**User:** "Compare the evaluator grades with actual grades for WorkSubmissions01"

**Claude:**
1. Loads grades from outputs/grades.xlsx → Found 36 students
2. Scans WorkSubmissions01 folder → Found 36 folders
3. Extracts grades from 36 PDFs → Successfully parsed 34/36
4. Calculates comparison metrics
5. Generates Excel report
6. "Comparison complete!"
   - Mean difference: -7.8 points (evaluator grades lower)
   - Correlation: 0.85 (strong)
   - 28/36 students within ±10 points
   - Saved to outputs/grade_comparison.xlsx

## Interpretation Guide

### Correlation Coefficient

- **0.9-1.0**: Excellent agreement (evaluator very accurate)
- **0.7-0.9**: Strong correlation (evaluator reliable)
- **0.5-0.7**: Moderate correlation (some reliability)
- **<0.5**: Weak correlation (evaluator needs improvement)

### Mean Difference

- **Positive**: Evaluator grades higher than actual (over-grading)
- **Negative**: Evaluator grades lower than actual (under-grading)
- **Close to 0**: Well-calibrated evaluator

### Mean Absolute Error

- **<5 points**: Excellent accuracy
- **5-10 points**: Good accuracy
- **10-15 points**: Fair accuracy
- **>15 points**: Needs calibration

## Tips for Best Results

1. **Ensure PDF quality** - Clear, text-based PDFs parse better than scanned images
2. **Consistent PDF format** - All PDFs should use same grade format
3. **Match student IDs** - Verify student IDs are consistent between evaluator and PDFs
4. **Review discrepancies** - Manually check large differences to understand why
5. **Use for calibration** - Adjust evaluation criteria based on comparison results

## Dependencies

The comparison script requires:
- Python 3.8+
- openpyxl (Excel generation)
- PyPDF2 or pdfplumber (PDF parsing)
- pandas (data processing)
- numpy (statistical calculations)

Install with:
```bash
pip install openpyxl PyPDF2 pdfplumber pandas numpy scipy
```
